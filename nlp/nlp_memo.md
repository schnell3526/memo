# 自然言語処理メモ

## 自然言語処理とは

**自然言語**とは日本語や英語をはじめとして我々が普段読み書きする言語のこと
**自然言語処理**(Natural Language Processing, **NLP**)とは, 自然言語を計算機で処理する技術のこと

- 検索エンジン, 機械翻訳, 予測変換, スパムフィルタ, 音声アシスタント, 対話システム等んんに用いられる.

## 自然言語処理技術の要素

- 形態素解析
  -> 文章を単語に分割する技術. 日本語で難しく英語で容易

- 単語の分散表現
  -> 文書内での関係性を踏まえて単語をベクトル化する技術

- 再帰型ニューラルネットワーク(RNN)
  -> 時系列を扱うのが得意なニューラルネットワークの一種

- Seq2Seq
  -> RNNをベースにした文章などを生成可能な

## [Transformer](https://arxiv.org/abs/1706.03762)の概要

RNNにはデータを並列で処理できないため学習時間が長い点や長時間の関係性を捉えるのが苦手なため文脈を考慮するのが難しいという問題点がある.

- transformerとは
  -> 2017年に導入されたDLモデルで主に自然言語処理の分野で使用される

  -> RNNと同様に自然言語などの時系列データを処理するように設計されているがRNNで用いる再帰, CNNで用いる畳み込みは使わない

  -> Attention層のみで構築される. Attentionは時系列データの特定の部分に注意を向けるように学習させていく方法

  -> 翻訳やテキストの要約など様々なタスクで利用可能

  -> 並列化が容易であり訓練時間を大きく削減できる

## [BERT](https://arxiv.org/abs/1810.04805)(Birdirectional Encorder Representation from Transformers)の概要

2018年の後半にgoogleから発表された. Pre-training済みのモデルをタスクに合わせてFine-tuningして利用する. 他種のタスクにFine-tuning可能なモデルである.

- 事前学習
  -> Transformerが文章から文脈を双方向に学習する
  -> Masked Language ModelおよびNext Sentence Predictionによる双方向学習

- ファインチューニング
  -> 事前学習により得られたパラメータを初期値としてラベル(正解データ)付きのデータでファインチューニングを行う

- Masked Langage model
  -> 文章から特定の単語を15%ランダムに選び\[MASK]トークンに置き換える
  -> \[MASK]の単語を前後の文脈から予測する

- Next Sentence Prediction
  -> 2つの文章に関係があるかどうかを判定する
  -> 後ろの文章を50%の確率で無関係な文章に置き換える
  -> 後ろの文章が意味的に適切であればisNext, そうでなければNotNextの判定

